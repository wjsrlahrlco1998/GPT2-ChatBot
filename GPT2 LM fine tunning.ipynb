{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b369da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.7.1+cu110\n",
    "!pip install fastai==2.4\n",
    "!pip install transformers==4.10.2\n",
    "!pip install BentoML==0.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8895c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
    "from fastai.text.all import *\n",
    "import fastai\n",
    "import re\n",
    "\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "print( fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì €, ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>') \n",
    "model = AutoModelWithLMHead.from_pretrained(\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbed25",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "#test tokenizer\n",
    "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))\n",
    "\n",
    "#test model ouput\n",
    "text = \"\"\"ìœ„ì¹˜ì¶”ì  ì „ìì¥ì¹˜(ì „ìë°œì°Œ) í›¼ì† ì „í›„ë¡œ ì—¬ì„± 2ëª…ì„ ì‡ë‹¬ì•„ ì‚´í•´í•œ \"\"\"\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True\n",
    "                        )\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ë°ì´í„°ë¡œë“œ\n",
    "# ë¬¸ì¥ ë°ì´í„° ë¡œë“œ: ë¬¸ì¥ì´ í•œì¤„ì”© ì¡´ì¬í•˜ëŠ” ë°ì´í„°\n",
    "sents = pd.read_csv(\"./GPT2 í•™ìŠµ ë°ì´í„° ìƒì„±/clean_data/sents.csv\")\n",
    "sents = sents.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce679116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ 1\n",
    "def clean_sent_apply(x):\n",
    "    x = re.sub(r'<.*>', '', x)\n",
    "    x = re.sub(r'\\n', ' ', x)\n",
    "    x = re.sub(r' +', ' ', x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "sents['ë¬¸ì¥'] = sents.ë¬¸ì¥.apply(clean_sent_apply)\n",
    "sents = sents.loc[sents.ë¬¸ì¥.map(len) < 30].reset_index(drop=True)\n",
    "sents = sents.ë¬¸ì¥.to_list()\n",
    "lines = sents[:300]\n",
    "lines = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6644a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model input output tokenizer\n",
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        toks = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n",
    "\n",
    "#split data\n",
    "train=lines[:int(len(lines)*0.9)]\n",
    "test=lines[int(len(lines)*0.9):]\n",
    "splits = [[0],[1]]\n",
    "\n",
    "#init dataloader\n",
    "tls = TfmdLists([train,test], TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\n",
    "batch,seq_len = 8,256\n",
    "dls = tls.dataloaders(bs=batch, seq_len=seq_len)\n",
    "# dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt2 ouput is tuple, we need just one val\n",
    "class DropOutput(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]\n",
    "        \n",
    "        \n",
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()\n",
    "lr=learn.lr_find()\n",
    "print(lr)\n",
    "learn.fit_one_cycle(5, lr)\n",
    "# learn.fine_tune(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"ìœ„ì¹˜ì¶”ì  ì „ìì¥ì¹˜(ì „ìë°œì°Œ) í›¼ì† ì „í›„ë¡œ ì—¬ì„± 2ëª…ì„ ì‡ë‹¬ì•„ ì‚´í•´í•œ \"\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None].cuda()\n",
    "preds = learn.model.generate(inp,\n",
    "                           max_length=128,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           repetition_penalty=2.0,       \n",
    "                           use_cache=True\n",
    "                          ) \n",
    "tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.save_pretrained(\"kogpt2novel_backup\")\n",
    "model.push_to_hub(\"kogpt2novel\")\n",
    "tokenizer.push_to_hub(\"kogpt2novel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
