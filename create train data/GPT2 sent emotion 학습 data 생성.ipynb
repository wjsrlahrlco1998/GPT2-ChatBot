{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5b2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "669efae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5415851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f60b8503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['우리 두 사람은 창밖을 향해 나란히 서서 그 해괴망측한 춤을 추었다. 내가 “자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['위저우저우는 마지막으로 약속을 어기고 이별하는 것으로 그를 인정사정없이 괴롭혔다...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  ['우리 두 사람은 창밖을 향해 나란히 서서 그 해괴망측한 춤을 추었다. 내가 “자...\n",
       "1  ['위저우저우는 마지막으로 약속을 어기고 이별하는 것으로 그를 인정사정없이 괴롭혔다..."
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"./raw_data/raw_sents.csv\")\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0977173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.drop(raw_data.loc[raw_data.comment == '[]',].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "949b92d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['우리 두 사람은 창밖을 향해 나란히 서서 그 해괴망측한 춤을 추었다. 내가 “자 봐봐. 이렇게 하는 거야.” 하며 시범을 보이자 그녀의 웃음소리가 병실 가득 메아리쳤다. 이 춤은 반한 여자 앞에서 출 만한 춤이 결코 아니었다. 그러나 이렇게 해서라도 아까 본 피범벅 동영상을 머릿속에서 지울 수 있다면, 그녀의 두려움과 공포를 조금이나마 가라앉힐 수 있다면 나는 기꺼이 내 골반과 멋짐을 포기할 의사가 있었다.\\\\n공연이 클라이맥스에 다다랐을 때 그녀의 감탄이 터졌다.\\\\n“와, 너 잘 춘다. 진짜 인버뤄-브뤠이트 같아.”\\\\n일단 웃었다. 마이클 잭슨이나 엘비스 프레슬리 정도의 칭찬으로 들었다. 인버뤄브뤠이트가 ‘무척추동물’이라는 영어 단어라는 것을 나중에 알았다.\\\\n어쨌든 이 춤은 그런 의미였다. ‘매우 큰 충격’을 ‘더 큰 충격’으로 잊어버리게 만들려는 시도. 결과를 알 수 없기에 더 무시무시하게 다가오는 내일의 공포를 다른 식으로 잊는 방법을 확실히는 모른다. 다만 내일은 우리의 끝이 아니며 우리의 끝은 그녀의 죽음이 아니라는 것만 생각할 뿐이다.\\\\n“그럼, 작별 인사를 해볼까?”\\\\n그녀는 호흡을 가다듬으며 흘러내린 앞머리를 귀 뒤로 깔끔하게 넘기고 내 앞에 섰다. 나는 손을 뻗어 빙긋 웃는 그녀의 턱에 말라붙어 있는 귤껍질을 떼어주었다.\\\\n벌써 일주일째 매일같이 작별 인사를 해왔다.\\\\n“그동안 즐거웠어. 고마워. 안녕.”\\\\n오늘이라고 해서 특별할 것도 없는 작별 인사였지만 인사를 끝내고 나면 언제나 할 말이 더 남아 있는 것처럼 한참이나 뜸을 들였다.\\\\n듣고 싶지만 듣지 못할 걸 알고 있었다. 그렇다고 해서 조급하거나 안달 날 것도 없었다. 그녀가 말을 아끼는 이유는 아마 우리에게 내일이 남아 있는 까닭이며 슬픔은 그보다 더 깊고 비밀스러운 곳에 있을 것이기 때문이다.\\\\n그녀 정수리에 손을 얹었다. ‘힘내.’라든가 ‘잘 될 거야.’라든가 ‘응원할게.’ 같은 말은 지금 내 기분을 조금도 전달할 수 없었다. 물에 빠진 각설탕처럼 내일이면 단물만 남긴 채 사라질 것만 같아서 나는 매일 그녀의 흔적을 같은 방식으로 잡아두었다. 정수리에 손바닥을 올리는 방식.\\\\n냄새는 음악처럼 순간을 기억한다. 그녀가 나의 시도를 눈치채지 못하도록 정수리에 올렸던 손바닥을 코에 대고 숨을 들이마셨다. 순식간에 밀려들어 온 체향(體香)은 아스라이 멀어지는 그녀를 내 앞에 데려다 놓았다.\\\\n--- 「프롤로그」 중에서\\\\n\\\\n“제3조 임금. 계약과 동시에 계약금 3억 원을 지불하며, 10일 기준으로 300만 원씩 추가 지급….”\\\\n계약서를 들고 있던 손에 힘이 들어갔다. 계약금 3억? 이 여자가 진짜 정신이 나갔나. 흘러나오는 클래식 음악에 심취한 듯 눈을 지그시 감고 있던 그녀는 내 시선을 느꼈는지 살며시 속눈썹을 들어 올리고 “왜? 돈이 적어?” 하고 물었다. 나는 뭐라고 대꾸하려다가 입을 다물어 버렸다. 300만 원을 껌값 취급한다면 이쪽에서도 그렇게 여길 작정이다.\\\\n구겨진 계약서를 들고 마저 읽어 내려갔다.\\\\n“제4조 근로 범위. 갑의 남자 친구 역할로서 연인 관계에 이루어지는 모든 일을 함께한다. 단, 갑이 허락하지 않은 스킨십을 할 경우 계약 위반으로 처리….”\\\\n또다시 읽는 걸 멈추고 앞에 앉은 여자를 빤히 보았다. 이 부분은 도저히 그냥 넘어갈 수가 없다.\\\\n“너 뭔가 잘 모르나 본데. 연인 관계를 전제로 이루어지는 일이란 오직 스킨십밖에 없어. 스킨십을 제외한 다른 모든 일은 연인 관계가 아니어도 할 수 있거든? 이거 계약 내용이 엉망진창인 거 알아?”\\\\n“별로 중요하지도 않아. 할 거야, 말 거야?”\\\\n\\\\n그 후로도 계약서는 온통 진흙탕이었다. 곳곳에 발이 빠져 매끄럽게 읽어나갈 부분이 거의 없었다. 근로 시간이 24시간이라고 명시되어 있는 부분에서 다섯 번째로 읽는 것을 멈췄을 때 목덜미가 뻣뻣해지는 걸 느꼈다. 을사조약 이래 가장 불합리한 계약이었다. 휴일도 없었다. 아니, 있었지만 갑의 재량에 따른다는 부분까지 읽고 없다는 의미로 해석을 마쳤다.\\\\n“그래서, 할 거야? 말 거야?”\\\\n마음 같아서는 손에 들고 있는 종이를 찢어버리고 밖으로 나가고 싶었지만 일단 사인하면 계약금 3억 원, 열흘에 300만 원이다. 감정에 치우치지 않고 이성적으로 생각을 해본다면 못 할 것도 없었다. 비위 맞추기 까다로워 보이긴 해도 나는 전문가니까. 눈 딱 감고 100일만 버텨보기로 마음먹었다.\\\\n“오케이, 콜!”\\\\n여자는 손때 하나 묻지 않은 명품 백에서 몽블랑 펜을 꺼냈다. 계약에 동의하면 사인하라는 말에 계약서 마지막 장을 마저 훑었다. “계약에 대한 일체 내용은 비밀을 유지한다. 을이 계약 내용을 위반하거나, 일방적 해지를 원할 경우 계약금을 세 배로 반환한다.”라는 내용이 적혀 있었다. 끝이 아니었다. 제일 마지막에 인쇄된 글자가 아닌 손 글씨로 적힌 문장이 눈에 띄었다.\\\\n‘을이 갑에게 마음을 뺏기는 경우 계약은 해지되고, 계약금은 100% 반환한다.’\\\\n흠칫 놀랐다. 야무진 글씨체를 보니 직접 쓴 것 같았다. 마지막 조항을 보고 전혀 동요하지 않았다고 하면 거짓말이다. 찰나의 순간이지만 망설임이 분명히 있었다. 그러나 내색하지 않았다. 그럴 일은 절대로 없을 거라며 스스로에게 다짐하고 계약서에 주저 없이 사인했다. 계약서를 받아 든 여자는 그제야 내 이름을 확인했다.\\\\n--- 「1. 첫 만남」 중에서\\\\n\\\\n공항으로 가는 리무진 안에서 나는 혹시나 하는 마음에 다시 한번 물었다.\\\\n“진짜 제주도 가는 거 아니지?”\\\\n“가고 있잖아, 지금.”\\\\n“넌 무슨 제주도를 택시 타고 안국역 외치듯이 가냐? 겨우 손바닥만 한 가방 하나 달랑 들고 편의점에 껌 사러 가듯이 간다고?”\\\\n“방어회만 먹고 올 건데, 뭐. 해외도 아니고.”\\\\n“그러니까 내 말은 그 방어회, 저기 보이지? 저 횟집에도 파는데 굳이 제주도까지 가서 먹을 필요가 있냐는 말이야. 아까 그 물고기가 바로 저 물고기야.”\\\\n리무진은 횟집 앞을 유유히 지나쳤다. 그녀는 손으로 우아하게 머리를 쓸어 넘기며 말했다.\\\\n“넌 모르는구나? 음식은 음식 자체가 중요한 게 아니라 분위기가 더 중요한 거야. 예를 들어 와인을 마신다고 생각해 봐. 지하 주차장에서 종이컵에 마시는 거랑 야경이 끝내주는 스카이라운지에서 크리스털 잔에 마시는 거랑 똑같다고 생각하니? 난 배를 채우기 위해 음식을 먹는 게 아니야. 굳이 방어회가 아니어도 먹을 거라면 얼마든지 있지만, 오늘 내가 먹고 싶은 건 ‘제주 바다가 보이는 횟집의 방어회’야. 알겠니? 이제 잔소리 말고 따라오도록.”\\\\n‘지하 주차장에서 어떤 미친놈이 와인을 마셔? 비유가 너무 제멋대로인 거 아닌가?’\\\\n체념한 나는 의자에 머리를 기댔다. 어제부터 빠르게 내려놓는 연습 중이다. 어차피 ‘갑’이 하자는 대로 따를 수밖에 없는 입장이니 떠들어 봤자 입만 아프다.\\\\n‘을’은 그녀의 캐시미어 머플러, 펜디 선글라스, 샤넬 코트와 함께 제주행 비행기에 짐짝처럼 실렸다. 복도 건너, 주디라고 불리는 임은미 실장과 같이 앉아 있는 그녀를 보았다. 깔깔거리며 떠드는 모습은 영락없이 소녀 같다. 곧 죽을 사람처럼 보이지 않았다. 하긴, 지금 이륙하는 비행기가 엔진 고장으로 추락한다면 여기 있는 사람 모두 죽을 수 있겠지만 다가올 죽음에 두려워하는 사람은 적어도 내가 볼 땐 아무도 없었다.\\\\n어딜 가도 개인 비서가 동행하니 단둘이 있을 일은 없겠다 싶은 생각에 안도하면서 한편으로는 아쉬운 마음도 들었다. 그럴 거면 애초에 내가 왜 필요한 건가 싶기도 하고. 돈 받은 만큼 뭔가 일을 해야겠다는 결심이 섰다. 그래야 100일 뒤에 떳떳하게 3억 원을 쓸 수 있을 테니. 자리에서 몸을 일으켰다.\\\\n“임 실장님, 잠시 자리 좀 바꿔주세요.”\\\\n임 실장은 기꺼이 나와 자리를 바꾸어 주었다. 옆자리에 앉는 나를 의아한 눈으로 보는 그녀에게 적당히 핑계를 댔다.\\\\n“300만 원짜리 운동화도 이렇게 팽개쳐 놓진 않아. 내 몸값이 얼만데. 3억짜리 남친 제대로 활용 안 할 거면 지금이라도 물리든가. 돈 돌려줄게. 그리고 생각해 보니까 네 말도 맞는 것 같아서. 너나 나나, 이 비행기에 타고 있는 사람들 모두 언제 죽을지 모르잖아. 비행기가 추락해서 우리 둘이 동시에 죽어버릴 수도 있다고 생각하니까 나도 좀 열심히 살아봐야겠다, 뭐 그런 생각이 드네.”\\\\n역시 아무 말이나 했다. 그녀는 내 말에 야무지게 반박했다.\\\\n“비행기 추락? 죽는 방법치고는 나쁘지 않지만 신은 그런 식으로 스토리를 전개하지 않아. 일단 오늘은 방어회를 먹을 예정이니까.”\\\\n열심히 해보겠다는 의지를 단박에 꺾어버리는 그녀의 말에 어쩐지 약이 올랐다.\\\\n“네가 신을 잘 몰라서 그러나 본데, 그 양반이 하는 일은 대체로 맥락 없이 전개되는 경우가 더 많아.”\\\\n“신과 친한 척하는 거야?”\\\\n“전혀.”\\\\n“비극이 어째서 비극인 줄 알아? 주인공이 죽어서 비극인 게 아니라 죽는 방법이 아름답지 않았기 때문에 비극인 거야.”\\\\n“아름답게 죽는 방법도 있냐?”\\\\n“원한을 품은 타인의 칼에 찔려 죽는 일은 대체로 비극이지. 아프잖아.”\\\\n“총으로 자살하는 건?”\\\\n“그것도 비극이지. 아프고 끔찍하니까.”\\\\n“그럼 어떻게 죽어야 비극이 아닌 건데?”\\\\n“고통 없는 죽음이 아름다운 죽음이야. 참을 수 있는 고통을 포함해서.”\\\\n연인 사이라 하기에 우리의 대화는 전혀 로맨틱하지 않았다. 우리는 어떤 방법으로 죽는 게 가장 ‘아름다운’ 혹은 ‘확실한’ 죽음 방법인가를 주제로 열띤 토론을 벌였다. 그녀는 라이너 마리아 릴케가 사랑하는 여인에게 선물하기 위해 장미꽃을 꺾다가 가시에 찔려 죽었다는 얘기를 마치 본인의 전 남친 얘기인 양 떠들어 댔고 나는 그걸 구경했다.\\\\n그녀가 나에게 “그런 죽음에 대해 어떻게 생각해?”라고 아련한 표정으로 묻길래 나는 이해되지 않는 부분에 대해 나름 진지하게 물었다.\\\\n“가시에 찔려 죽으려면 가시가 30센티미터 이상의 거대 가시거나 아니면 최소 500번 이상 찔려야 되지 않냐? 겁나 아팠겠다. 운도 더럽게 없네.”\\\\n아무 말 없이 노려보는 그녀의 눈빛에 입을 다물었다. 이게 바로 비극이었다. 때마침 스튜어디스가 지나갔다. 곤란한 타이밍에 기다렸다는 듯 물을 가져다준 스튜어디스에게 이 물이 제주 삼다수냐는 실없는 농담을 건넸고 수줍게 웃는 스튜어디스와 몇 마디 주고받는 사이 천만다행으로 릴켄지 랄켄지 뭔지 모를 남자의 죽음은 잊혔다.\\\\n짧은 비행시간 동안 우리의 대화 속엔 다양한 죽음의 방법들이 등장했지만 역시나 ‘아프지 않게’ 혹은 ‘한 방에’ 죽는 것이 가장 좋은 방법이라는 결론이 내려졌다. 그런 의미로 병들어 죽는 것보다는 사고로 죽는 게 차라리 나을 거라는 쪽에 동의했고, 갑작스러운 핵폭발이라든가 예기치 못한 운석 충돌이 가장 좋은 죽음이라고 합의를 보았다. 아무래도 혼자 죽는 건 외롭고 무서우니까 다 같이 죽자는 데에서 처음으로 마음이 맞은 것이다.\\\\n--- 「4. 녹여 먹어요」 중에서\\\\n\\\\n파를 다듬다 말고 아예 몸을 돌리고 서서 그녀를 유심히 관찰했다. 그동안 인간적 관계를 맺는데 많은 시간이나 노력을 쏟을 필요가 없었던 나는 타인에 관한 관심이 별로 없었다. 그들에게 맞추지 않아도, 내 멋대로 말하고 행동해도, 가끔은 어떤 말이나 행동을 전혀 하지 않아도 원하는 것을 얻을 수 있었다.\\\\n오히려 혼자 있는 걸 좋아하는 편이라 내가 관심을 갖고 연구해야 할 대상은 IPTV 서비스, 침대의 안락함, 배달 음식의 범위, 수돗물의 안정성 등이지 여자에 대한 이해는 아니었다. 물론 여자와의 관계는 생계의 원천이라 비즈니스를 위해 어쩔 수 없이 만나야 하지만 그녀들은 나의 호기심을 자극하지 못했다. 나쁜 여자는 나를 귀찮게 했고 착한 여자는 나를 질리게 했다. 나에게 여자는 보편적이면서도 일반화가 가능한 존재였다.\\\\n그러나 은제이에 대해 알고 싶은 범위는 내가 모르는 20년의 과거, 작은 습관, 사소한 버릇 등을 뛰어넘어 그 이상이었다. 지나치게 관심이 생겨버렸다. 그녀가 소유하고 있는 모든 물건의 출처와 역사, ‘어을티메이를리’를 발음할 때 혀와 입술이 움직이는 모양, 그녀가 지금껏 만나온 모든 사람들에 대해 알고 싶어졌다. 새삼스럽게 계약서 마지막 글귀가 떠올랐다.\\\\n‘갑에게 마음을 뺏기는 경우 계약금 100% 반환.’\\\\n정신 차리자 전세계. 96일 뒤에 떳떳하게 그 돈을 쓰려면 지금 역할에만 충실하자.\\\\n손으로 뺨을 착착 두드린 뒤 다시 몸을 돌려 파 껍질을 마저 깠다. 갓 지은 따뜻한 밥을 도시락에 퍼 담고 반찬과 국을 담았다.\\\\n나는 도시락과 수저, 작은 생수 한 병을 종이 가방에 담으며 그녀에게 물었다.\\\\n“넌 죽기 전에 이런 노동을 왜 해보고 싶었던 거야?”\\\\n그녀는 하던 일을 멈추고 나를 보았다.\\\\n“노동? 도시락 만들어서 배달하는 게 노동이야?”\\\\n“노동 아니면 뭔데?”\\\\n“당연히 사랑이지.”\\\\n사랑이라고 대답한 그녀는 매우 충격적이라는 표정으로 나에게 되물었다.\\\\n“너 설마… 오늘 하루 종일 노동했다고 생각하는 거야?”\\\\n“그럼 넌? 하루 종일 사랑했다고 생각하는 거냐?”\\\\n내가 궁금했던 건 단순히 ‘죽기 전에 도시락을 왜 싸보고 싶었는가?’ 하는 거였다. 그러나 방향은 완전히 틀어져 버렸다. 그녀는 도시락을 싼 게 아니었다. ‘사랑’을 했다는 것이 문제의 발단이었다.\\\\n그녀는 얼굴이 붉어지도록 흥분해서 말했다.\\\\n“난 오늘 최선을 다해서 사랑을 했어. 내 마음을 도시락 하나하나에 담아내며 너무 감동받아서 눈물이 날 뻔했다고!”\\\\n“인생을 너무 감성적으로 사는 것 같다? 무슨 드라마 주인공도 아니고. 희한한 논리를 갖고 사네.”\\\\n“너야말로 이상해. 우리가 전달하는 건 밥이 아니야. 사랑이야!”\\\\n눈썹에 잔뜩 힘을 주고 “사랑이야!” 하는 말에 웃음이 터졌다. 웃을 상황이 아니었는데 웃음이 나는 건 어쩔 수 없었다. 그녀의 얼굴은 아까보다 더 심각하게 구겨지고 귀여워졌다. 이제는 그녀가 도시락을 싼 이유보다 이 대화의 끝이 궁금해졌다.\\\\n“웃지 마. 사람은 밥만으로는 살 수 없어. 고독한 사람들에게는 관심과 사랑을 나눠주는 게 밥보다 더 중요해. 내 도시락을 받은 분들은 분명 나의 사랑을 느낄 수 있을 거라고!”\\\\n“사랑이 밥보다 중요하면, 밥 안 먹고도 살 수 있겠네. 그럼 밥은 왜 했냐? 빈 도시락에 ‘사랑만 담았어요.’ 하고 건네주지.”\\\\n그녀를 놀리는 것도 재미있고, 콩 주머니 주고받듯 하는 대화도 재미있었다. 웃음을 참으며 건성으로 말하는 내 태도에 결국 그녀의 언성이 높아졌다.\\\\n“무슨 소리를 하는 거야. 너 애가 왜 그렇게 삐딱하니?”\\\\n“나 원래 삐딱한 놈이야. 알지도 못하는 노인네들 밥해 먹이려고 몇백만 원어치씩 장 봐본 적 없는 나는 사랑이 뭔지 몰라서 삐딱하다. 됐냐?”\\\\n대화를 이런 식으로 전개하려고 시작한 건 아니었는데 마지막에 살짝 본심이 드러났다. 하루 종일 함께 있으면서 겨우 도시락 만들기밖에 하지 않았다는 데에서 오는 서운함이 애매한 방향으로 꼬였다. 남자 친구 역할 하라면서 3억 원이나 입금해 놓고 겨우 뿌리채소 다듬는 데 하루를 쓰게 하다니 인력 낭비다. 그래서 나는 끝까지 나 잘났다 하고 버티려고 했다. 그러나 그녀 눈에 눈물이 고이는 걸 보고 정신이 번쩍 들었다. 뒤통수를 한 대 맞은 기분이었다.']\""
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.comment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ea6c3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['comment'] = raw_data.comment.apply(lambda x : x.replace('[', '').replace(']', '').replace('\\\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "23d48ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['comment'] = raw_data.comment.apply(lambda x : x[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "63e84d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.rename(columns={'comment':'문장'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "198526eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리 두 사람은 창밖을 향해 나란히 서서 그 해괴망측한 춤을 추었다. 내가 “자 봐봐. 이렇게 하는 거야.” 하며 시범을 보이자 그녀의 웃음소리가 병실 가득 메아리쳤다. 이 춤은 반한 여자 앞에서 출 만한 춤이 결코 아니었다. 그러나 이렇게 해서라도 아까 본 피범벅 동영상을 머릿속에서 지울 수 있다면, 그녀의 두려움과 공포를 조금이나마 가라앉힐 수 있다면 나는 기꺼이 내 골반과 멋짐을 포기할 의사가 있었다.공연이 클라이맥스에 다다랐을 때 그녀의 감탄이 터졌다.“와, 너 잘 춘다. 진짜 인버뤄-브뤠이트 같아.”일단 웃었다. 마이클 잭슨이나 엘비스 프레슬리 정도의 칭찬으로 들었다. 인버뤄브뤠이트가 ‘무척추동물’이라는 영어 단어라는 것을 나중에 알았다.어쨌든 이 춤은 그런 의미였다. ‘매우 큰 충격’을 ‘더 큰 충격’으로 잊어버리게 만들려는 시도. 결과를 알 수 없기에 더 무시무시하게 다가오는 내일의 공포를 다른 식으로 잊는 방법을 확실히는 모른다. 다만 내일은 우리의 끝이 아니며 우리의 끝은 그녀의 죽음이 아니라는 것만 생각할 뿐이다.“그럼, 작별 인사를 해볼까?”그녀는 호흡을 가다듬으며 흘러내린 앞머리를 귀 뒤로 깔끔하게 넘기고 내 앞에 섰다. 나는 손을 뻗어 빙긋 웃는 그녀의 턱에 말라붙어 있는 귤껍질을 떼어주었다.벌써 일주일째 매일같이 작별 인사를 해왔다.“그동안 즐거웠어. 고마워. 안녕.”오늘이라고 해서 특별할 것도 없는 작별 인사였지만 인사를 끝내고 나면 언제나 할 말이 더 남아 있는 것처럼 한참이나 뜸을 들였다.듣고 싶지만 듣지 못할 걸 알고 있었다. 그렇다고 해서 조급하거나 안달 날 것도 없었다. 그녀가 말을 아끼는 이유는 아마 우리에게 내일이 남아 있는 까닭이며 슬픔은 그보다 더 깊고 비밀스러운 곳에 있을 것이기 때문이다.그녀 정수리에 손을 얹었다. ‘힘내.’라든가 ‘잘 될 거야.’라든가 ‘응원할게.’ 같은 말은 지금 내 기분을 조금도 전달할 수 없었다. 물에 빠진 각설탕처럼 내일이면 단물만 남긴 채 사라질 것만 같아서 나는 매일 그녀의 흔적을 같은 방식으로 잡아두었다. 정수리에 손바닥을 올리는 방식.냄새는 음악처럼 순간을 기억한다. 그녀가 나의 시도를 눈치채지 못하도록 정수리에 올렸던 손바닥을 코에 대고 숨을 들이마셨다. 순식간에 밀려들어 온 체향(體香)은 아스라이 멀어지는 그녀를 내 앞에 데려다 놓았다.--- 「프롤로그」 중에서“제3조 임금. 계약과 동시에 계약금 3억 원을 지불하며, 10일 기준으로 300만 원씩 추가 지급….”계약서를 들고 있던 손에 힘이 들어갔다. 계약금 3억? 이 여자가 진짜 정신이 나갔나. 흘러나오는 클래식 음악에 심취한 듯 눈을 지그시 감고 있던 그녀는 내 시선을 느꼈는지 살며시 속눈썹을 들어 올리고 “왜? 돈이 적어?” 하고 물었다. 나는 뭐라고 대꾸하려다가 입을 다물어 버렸다. 300만 원을 껌값 취급한다면 이쪽에서도 그렇게 여길 작정이다.구겨진 계약서를 들고 마저 읽어 내려갔다.“제4조 근로 범위. 갑의 남자 친구 역할로서 연인 관계에 이루어지는 모든 일을 함께한다. 단, 갑이 허락하지 않은 스킨십을 할 경우 계약 위반으로 처리….”또다시 읽는 걸 멈추고 앞에 앉은 여자를 빤히 보았다. 이 부분은 도저히 그냥 넘어갈 수가 없다.“너 뭔가 잘 모르나 본데. 연인 관계를 전제로 이루어지는 일이란 오직 스킨십밖에 없어. 스킨십을 제외한 다른 모든 일은 연인 관계가 아니어도 할 수 있거든? 이거 계약 내용이 엉망진창인 거 알아?”“별로 중요하지도 않아. 할 거야, 말 거야?”그 후로도 계약서는 온통 진흙탕이었다. 곳곳에 발이 빠져 매끄럽게 읽어나갈 부분이 거의 없었다. 근로 시간이 24시간이라고 명시되어 있는 부분에서 다섯 번째로 읽는 것을 멈췄을 때 목덜미가 뻣뻣해지는 걸 느꼈다. 을사조약 이래 가장 불합리한 계약이었다. 휴일도 없었다. 아니, 있었지만 갑의 재량에 따른다는 부분까지 읽고 없다는 의미로 해석을 마쳤다.“그래서, 할 거야? 말 거야?”마음 같아서는 손에 들고 있는 종이를 찢어버리고 밖으로 나가고 싶었지만 일단 사인하면 계약금 3억 원, 열흘에 300만 원이다. 감정에 치우치지 않고 이성적으로 생각을 해본다면 못 할 것도 없었다. 비위 맞추기 까다로워 보이긴 해도 나는 전문가니까. 눈 딱 감고 100일만 버텨보기로 마음먹었다.“오케이, 콜!”여자는 손때 하나 묻지 않은 명품 백에서 몽블랑 펜을 꺼냈다. 계약에 동의하면 사인하라는 말에 계약서 마지막 장을 마저 훑었다. “계약에 대한 일체 내용은 비밀을 유지한다. 을이 계약 내용을 위반하거나, 일방적 해지를 원할 경우 계약금을 세 배로 반환한다.”라는 내용이 적혀 있었다. 끝이 아니었다. 제일 마지막에 인쇄된 글자가 아닌 손 글씨로 적힌 문장이 눈에 띄었다.‘을이 갑에게 마음을 뺏기는 경우 계약은 해지되고, 계약금은 100% 반환한다.’흠칫 놀랐다. 야무진 글씨체를 보니 직접 쓴 것 같았다. 마지막 조항을 보고 전혀 동요하지 않았다고 하면 거짓말이다. 찰나의 순간이지만 망설임이 분명히 있었다. 그러나 내색하지 않았다. 그럴 일은 절대로 없을 거라며 스스로에게 다짐하고 계약서에 주저 없이 사인했다. 계약서를 받아 든 여자는 그제야 내 이름을 확인했다.--- 「1. 첫 만남」 중에서공항으로 가는 리무진 안에서 나는 혹시나 하는 마음에 다시 한번 물었다.“진짜 제주도 가는 거 아니지?”“가고 있잖아, 지금.”“넌 무슨 제주도를 택시 타고 안국역 외치듯이 가냐? 겨우 손바닥만 한 가방 하나 달랑 들고 편의점에 껌 사러 가듯이 간다고?”“방어회만 먹고 올 건데, 뭐. 해외도 아니고.”“그러니까 내 말은 그 방어회, 저기 보이지? 저 횟집에도 파는데 굳이 제주도까지 가서 먹을 필요가 있냐는 말이야. 아까 그 물고기가 바로 저 물고기야.”리무진은 횟집 앞을 유유히 지나쳤다. 그녀는 손으로 우아하게 머리를 쓸어 넘기며 말했다.“넌 모르는구나? 음식은 음식 자체가 중요한 게 아니라 분위기가 더 중요한 거야. 예를 들어 와인을 마신다고 생각해 봐. 지하 주차장에서 종이컵에 마시는 거랑 야경이 끝내주는 스카이라운지에서 크리스털 잔에 마시는 거랑 똑같다고 생각하니? 난 배를 채우기 위해 음식을 먹는 게 아니야. 굳이 방어회가 아니어도 먹을 거라면 얼마든지 있지만, 오늘 내가 먹고 싶은 건 ‘제주 바다가 보이는 횟집의 방어회’야. 알겠니? 이제 잔소리 말고 따라오도록.”‘지하 주차장에서 어떤 미친놈이 와인을 마셔? 비유가 너무 제멋대로인 거 아닌가?’체념한 나는 의자에 머리를 기댔다. 어제부터 빠르게 내려놓는 연습 중이다. 어차피 ‘갑’이 하자는 대로 따를 수밖에 없는 입장이니 떠들어 봤자 입만 아프다.‘을’은 그녀의 캐시미어 머플러, 펜디 선글라스, 샤넬 코트와 함께 제주행 비행기에 짐짝처럼 실렸다. 복도 건너, 주디라고 불리는 임은미 실장과 같이 앉아 있는 그녀를 보았다. 깔깔거리며 떠드는 모습은 영락없이 소녀 같다. 곧 죽을 사람처럼 보이지 않았다. 하긴, 지금 이륙하는 비행기가 엔진 고장으로 추락한다면 여기 있는 사람 모두 죽을 수 있겠지만 다가올 죽음에 두려워하는 사람은 적어도 내가 볼 땐 아무도 없었다.어딜 가도 개인 비서가 동행하니 단둘이 있을 일은 없겠다 싶은 생각에 안도하면서 한편으로는 아쉬운 마음도 들었다. 그럴 거면 애초에 내가 왜 필요한 건가 싶기도 하고. 돈 받은 만큼 뭔가 일을 해야겠다는 결심이 섰다. 그래야 100일 뒤에 떳떳하게 3억 원을 쓸 수 있을 테니. 자리에서 몸을 일으켰다.“임 실장님, 잠시 자리 좀 바꿔주세요.”임 실장은 기꺼이 나와 자리를 바꾸어 주었다. 옆자리에 앉는 나를 의아한 눈으로 보는 그녀에게 적당히 핑계를 댔다.“300만 원짜리 운동화도 이렇게 팽개쳐 놓진 않아. 내 몸값이 얼만데. 3억짜리 남친 제대로 활용 안 할 거면 지금이라도 물리든가. 돈 돌려줄게. 그리고 생각해 보니까 네 말도 맞는 것 같아서. 너나 나나, 이 비행기에 타고 있는 사람들 모두 언제 죽을지 모르잖아. 비행기가 추락해서 우리 둘이 동시에 죽어버릴 수도 있다고 생각하니까 나도 좀 열심히 살아봐야겠다, 뭐 그런 생각이 드네.”역시 아무 말이나 했다. 그녀는 내 말에 야무지게 반박했다.“비행기 추락? 죽는 방법치고는 나쁘지 않지만 신은 그런 식으로 스토리를 전개하지 않아. 일단 오늘은 방어회를 먹을 예정이니까.”열심히 해보겠다는 의지를 단박에 꺾어버리는 그녀의 말에 어쩐지 약이 올랐다.“네가 신을 잘 몰라서 그러나 본데, 그 양반이 하는 일은 대체로 맥락 없이 전개되는 경우가 더 많아.”“신과 친한 척하는 거야?”“전혀.”“비극이 어째서 비극인 줄 알아? 주인공이 죽어서 비극인 게 아니라 죽는 방법이 아름답지 않았기 때문에 비극인 거야.”“아름답게 죽는 방법도 있냐?”“원한을 품은 타인의 칼에 찔려 죽는 일은 대체로 비극이지. 아프잖아.”“총으로 자살하는 건?”“그것도 비극이지. 아프고 끔찍하니까.”“그럼 어떻게 죽어야 비극이 아닌 건데?”“고통 없는 죽음이 아름다운 죽음이야. 참을 수 있는 고통을 포함해서.”연인 사이라 하기에 우리의 대화는 전혀 로맨틱하지 않았다. 우리는 어떤 방법으로 죽는 게 가장 ‘아름다운’ 혹은 ‘확실한’ 죽음 방법인가를 주제로 열띤 토론을 벌였다. 그녀는 라이너 마리아 릴케가 사랑하는 여인에게 선물하기 위해 장미꽃을 꺾다가 가시에 찔려 죽었다는 얘기를 마치 본인의 전 남친 얘기인 양 떠들어 댔고 나는 그걸 구경했다.그녀가 나에게 “그런 죽음에 대해 어떻게 생각해?”라고 아련한 표정으로 묻길래 나는 이해되지 않는 부분에 대해 나름 진지하게 물었다.“가시에 찔려 죽으려면 가시가 30센티미터 이상의 거대 가시거나 아니면 최소 500번 이상 찔려야 되지 않냐? 겁나 아팠겠다. 운도 더럽게 없네.”아무 말 없이 노려보는 그녀의 눈빛에 입을 다물었다. 이게 바로 비극이었다. 때마침 스튜어디스가 지나갔다. 곤란한 타이밍에 기다렸다는 듯 물을 가져다준 스튜어디스에게 이 물이 제주 삼다수냐는 실없는 농담을 건넸고 수줍게 웃는 스튜어디스와 몇 마디 주고받는 사이 천만다행으로 릴켄지 랄켄지 뭔지 모를 남자의 죽음은 잊혔다.짧은 비행시간 동안 우리의 대화 속엔 다양한 죽음의 방법들이 등장했지만 역시나 ‘아프지 않게’ 혹은 ‘한 방에’ 죽는 것이 가장 좋은 방법이라는 결론이 내려졌다. 그런 의미로 병들어 죽는 것보다는 사고로 죽는 게 차라리 나을 거라는 쪽에 동의했고, 갑작스러운 핵폭발이라든가 예기치 못한 운석 충돌이 가장 좋은 죽음이라고 합의를 보았다. 아무래도 혼자 죽는 건 외롭고 무서우니까 다 같이 죽자는 데에서 처음으로 마음이 맞은 것이다.--- 「4. 녹여 먹어요」 중에서파를 다듬다 말고 아예 몸을 돌리고 서서 그녀를 유심히 관찰했다. 그동안 인간적 관계를 맺는데 많은 시간이나 노력을 쏟을 필요가 없었던 나는 타인에 관한 관심이 별로 없었다. 그들에게 맞추지 않아도, 내 멋대로 말하고 행동해도, 가끔은 어떤 말이나 행동을 전혀 하지 않아도 원하는 것을 얻을 수 있었다.오히려 혼자 있는 걸 좋아하는 편이라 내가 관심을 갖고 연구해야 할 대상은 IPTV 서비스, 침대의 안락함, 배달 음식의 범위, 수돗물의 안정성 등이지 여자에 대한 이해는 아니었다. 물론 여자와의 관계는 생계의 원천이라 비즈니스를 위해 어쩔 수 없이 만나야 하지만 그녀들은 나의 호기심을 자극하지 못했다. 나쁜 여자는 나를 귀찮게 했고 착한 여자는 나를 질리게 했다. 나에게 여자는 보편적이면서도 일반화가 가능한 존재였다.그러나 은제이에 대해 알고 싶은 범위는 내가 모르는 20년의 과거, 작은 습관, 사소한 버릇 등을 뛰어넘어 그 이상이었다. 지나치게 관심이 생겨버렸다. 그녀가 소유하고 있는 모든 물건의 출처와 역사, ‘어을티메이를리’를 발음할 때 혀와 입술이 움직이는 모양, 그녀가 지금껏 만나온 모든 사람들에 대해 알고 싶어졌다. 새삼스럽게 계약서 마지막 글귀가 떠올랐다.‘갑에게 마음을 뺏기는 경우 계약금 100% 반환.’정신 차리자 전세계. 96일 뒤에 떳떳하게 그 돈을 쓰려면 지금 역할에만 충실하자.손으로 뺨을 착착 두드린 뒤 다시 몸을 돌려 파 껍질을 마저 깠다. 갓 지은 따뜻한 밥을 도시락에 퍼 담고 반찬과 국을 담았다.나는 도시락과 수저, 작은 생수 한 병을 종이 가방에 담으며 그녀에게 물었다.“넌 죽기 전에 이런 노동을 왜 해보고 싶었던 거야?”그녀는 하던 일을 멈추고 나를 보았다.“노동? 도시락 만들어서 배달하는 게 노동이야?”“노동 아니면 뭔데?”“당연히 사랑이지.”사랑이라고 대답한 그녀는 매우 충격적이라는 표정으로 나에게 되물었다.“너 설마… 오늘 하루 종일 노동했다고 생각하는 거야?”“그럼 넌? 하루 종일 사랑했다고 생각하는 거냐?”내가 궁금했던 건 단순히 ‘죽기 전에 도시락을 왜 싸보고 싶었는가?’ 하는 거였다. 그러나 방향은 완전히 틀어져 버렸다. 그녀는 도시락을 싼 게 아니었다. ‘사랑’을 했다는 것이 문제의 발단이었다.그녀는 얼굴이 붉어지도록 흥분해서 말했다.“난 오늘 최선을 다해서 사랑을 했어. 내 마음을 도시락 하나하나에 담아내며 너무 감동받아서 눈물이 날 뻔했다고!”“인생을 너무 감성적으로 사는 것 같다? 무슨 드라마 주인공도 아니고. 희한한 논리를 갖고 사네.”“너야말로 이상해. 우리가 전달하는 건 밥이 아니야. 사랑이야!”눈썹에 잔뜩 힘을 주고 “사랑이야!” 하는 말에 웃음이 터졌다. 웃을 상황이 아니었는데 웃음이 나는 건 어쩔 수 없었다. 그녀의 얼굴은 아까보다 더 심각하게 구겨지고 귀여워졌다. 이제는 그녀가 도시락을 싼 이유보다 이 대화의 끝이 궁금해졌다.“웃지 마. 사람은 밥만으로는 살 수 없어. 고독한 사람들에게는 관심과 사랑을 나눠주는 게 밥보다 더 중요해. 내 도시락을 받은 분들은 분명 나의 사랑을 느낄 수 있을 거라고!”“사랑이 밥보다 중요하면, 밥 안 먹고도 살 수 있겠네. 그럼 밥은 왜 했냐? 빈 도시락에 ‘사랑만 담았어요.’ 하고 건네주지.”그녀를 놀리는 것도 재미있고, 콩 주머니 주고받듯 하는 대화도 재미있었다. 웃음을 참으며 건성으로 말하는 내 태도에 결국 그녀의 언성이 높아졌다.“무슨 소리를 하는 거야. 너 애가 왜 그렇게 삐딱하니?”“나 원래 삐딱한 놈이야. 알지도 못하는 노인네들 밥해 먹이려고 몇백만 원어치씩 장 봐본 적 없는 나는 사랑이 뭔지 몰라서 삐딱하다. 됐냐?”대화를 이런 식으로 전개하려고 시작한 건 아니었는데 마지막에 살짝 본심이 드러났다. 하루 종일 함께 있으면서 겨우 도시락 만들기밖에 하지 않았다는 데에서 오는 서운함이 애매한 방향으로 꼬였다. 남자 친구 역할 하라면서 3억 원이나 입금해 놓고 겨우 뿌리채소 다듬는 데 하루를 쓰게 하다니 인력 낭비다. 그래서 나는 끝까지 나 잘났다 하고 버티려고 했다. 그러나 그녀 눈에 눈물이 고이는 걸 보고 정신이 번쩍 들었다. 뒤통수를 한 대 맞은 기분이었다.'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.문장[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1ee11758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-187-3d8d87713904>:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for raw_sents in tqdm_notebook(raw_data.문장.values):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87192f3d0e4f4244aab6d64feab3d437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents = []\n",
    "\n",
    "for raw_sents in tqdm_notebook(raw_data.문장.values):\n",
    "    sents.extend(kss.split_sentences(raw_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bc8bd1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9570"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ddbb9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df = pd.DataFrame({\"문장\" : sents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b3ad24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df.to_csv(\"./clean_data/sents.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50654f5",
   "metadata": {},
   "source": [
    "## KoBERT 파인튜닝 모델로 감정 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e918a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "85057981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 사용\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f5451943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 전처리 클래스\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7330fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT 모델 클래스(분류기)\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=4,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b4ac262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/lab06/세미프로젝트/GPT3 챗봇/ChatBot_GPT3 & 2/GPT2 학습 데이터 생성/.cache/kobert_v1.zip\n",
      "using cached model. /home/lab06/세미프로젝트/GPT3 챗봇/ChatBot_GPT3 & 2/GPT2 학습 데이터 생성/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "_, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cac8898a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kobert = torch.load('../model/KoBERT_sent_cls_model.pt')\n",
    "model_kobert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "58193ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a6219b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/lab06/세미프로젝트/GPT3 챗봇/ChatBot_GPT3 & 2/GPT2 학습 데이터 생성/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kobert = get_tokenizer()\n",
    "tok_kobert = nlp.data.BERTSPTokenizer(tokenizer_kobert, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7f2e39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_sentence):\n",
    "    \n",
    "    result_emo = None\n",
    "    \n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok_kobert, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    #model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model_kobert(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                test_eval.append(\"분노가\")\n",
    "                result_emo = '분노'\n",
    "            elif np.argmax(logits) == 1:\n",
    "                test_eval.append(\"행복이\")\n",
    "                result_emo = '행복'\n",
    "            elif np.argmax(logits) == 2:\n",
    "                test_eval.append(\"슬픔이\")\n",
    "                result_emo = '슬픔'\n",
    "            elif np.argmax(logits) == 3:\n",
    "                test_eval.append(\"중립이\")\n",
    "                result_emo = '중립' \n",
    "                \n",
    "        #print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")\n",
    "    \n",
    "    return result_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ebe7e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-201-8cde9c47f551>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for sent in tqdm_notebook(sents_df.문장.values):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a347b1047d76499ba094697dcd5fdf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotion_list = []\n",
    "\n",
    "# 문장 라벨링하기\n",
    "for sent in tqdm_notebook(sents_df.문장.values):\n",
    "    emotion_list.append(predict(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "71c77fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df['감정'] = emotion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2afcee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 : 이런 생각들을 하다가 나 자신에게 이런 질문을 자주 던졌다. ‘살아 있는데, 이 살아 있다는 것으로 무엇을 해야 할까?\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 무슨 말을 나눠야 할까?’ 그 질문을 중심으로 여러 생각들이 잔물결처럼 퍼져나갔다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그때 칼비노의 이야기도 생각나곤 했다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 흔하디흔한 시장 한구석이 특별해지는 것은 우리가 나 아닌 다른 누군가를 만났기 때문이고, 내가 아직 가보지 못한 곳이 있다는 것은 내가 아직 들어보지 못한 이야기가 있다는 뜻이 될 것이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나는 언어가 우리를 구해줄 수 있다고 믿고 있다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 새로운 생각, 새로운 말, 새로운 이야기가 있는 곳에서 새로운 사람이 태어난다고 믿고 있다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 수천 년 동안 인간 삶은 그렇게 변해왔다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그러니 나에게서 어떤 새로운 말도, 이야기도 나오지 않는 것, 이것이야말로 오늘 내가 가장 슬퍼해야 할 일이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그럼 이제 뭘 해야 할까?\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 살아 있는 자들이 진정으로 알고 싶어 하는 유일한 것은 자신의 미래다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 진정으로 만나고 싶어 하는 것은 좋은 미래다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 언어 공동체에 속하는 우리가 이 좋은 미래를 만나는 방법은 좋은 미래에 대해 많이 이야기하는 것이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 한 새로운 세계의 창조 앞에는 언제나 언어와 이야기가 있어왔다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그러니 살아 있는 자의 심장에서 나온 살아 있는 이야기는 우리 모두를 살아 있게 하는 데 필수적이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 한 사람의 좋은 이야기는 우리 모두의 이야기가 된다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 좋은 이야기는 우리의 내면 깊은 곳에 ‘부드럽게’ 각인되고 남아서 우리의 자아를 바꾼다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 세상에 존재하는 모든 부드러움\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 중 가장 믿을 수 없을 만큼 부드러운 것은 인간의 변화다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나는 공부를 많이 못 하고 부산으로 갔어.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 거기서 일을 하다가 할머니가 돌아가신 줄도 몰랐어.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 고향에 돌아와서야 돌아가신 걸 알았지.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그 후론 쭉 고향서 살았어.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 뱃사람이 될지도 모르겠다는 예감이 좀 있어서 뱃일을 배웠고 그 뒤로 바다와 고기 잡는 것에 푹 빠져 살았어.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 밤에는 배에 누워서 라디오를 듣곤 했어.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그리고 커피를 많이 마셨어.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 이상하게 배에서는 커피를 많이 마시게 돼.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그렇게 누워서 인간은 본질적으로 외로운 존재다, 이래 생각하고 살았지.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그래도 나 스스로 한 약속만은 친구처럼 어디든 같이 다녔어.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 내가 이런 말을 들으면 천국의 모습이 바뀔지도 궁금해. 내가 이제 살면 얼마나 더 살겠어.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 한 2~3년 남았을까?\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 내가 지금 듣는 것은 다시는 못 듣겠지.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 다시는 이야기도 못 나누겠지.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그런 걸 생각하면 아주 열성적으로 듣게 돼.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 귀가 배지근해지지.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 나를 좋아했던 사람이 준 찻잔을 손에 들고 그렇게 몇 그루 나무에 불과하지만 그래도 자연 속에 있으면 이상한 존재감이 생겨요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 누군가에게 좋은 사람이었던 나, 그런 게 조금씩 보여요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우울증이란 게 사실은 자신의 존재감도 느끼지 못하고 자신을 좋아하기 힘들어서 생겨난 일이라고 하잖아요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 제가 멘토라고 하니까 이상하지만 우울증에서 벗어나게 된 이야기는 알려드리고 싶었어요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 형이 누구인지 말하려면 우리의 마지막 날, 9월 10일 밤을 말하는 게 좋을 것 같아요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 형은 죽기전날 늦게까지 히스토리 채널에서 세계대전에 대한 다큐멘터리를 봤어요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 형은 역사에 관심이 많았어요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 하지만 형은 직장이 멀어서 일찍 자야 했어요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 제때 출근하려면 5시에는 일어나야 했거든요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그래서 형은 저에게 20분 뒤에 논쟁적인 인물이 나오니까 꼭 녹화해달라고 했어요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그게 형과 나눈 마지막 대화예요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우리 형은 합기도 같은 동양무술을 배우고 바닷가를 달리고 자전거를 타기 좋아했던, 활기차게 자기 삶을 즐겼던 사람이에요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우리 형, 그리고 그날 생을 떠난 2,977명은 모두 하나의 숫자가 아니에요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 모두 자기 인생 이야기가 있던 사람들이에요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 역사를 열정적으로 사랑했던 형은 이제 역사의 일부가 되고 있어요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 세상은 우릴 잊고 변하는데 우리는 그 일에 갇혀 있어요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우리는 계속 악몽을 꾸고 계속 소리 지르고 울어요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 벗어나야 한다고 하지만 잘 안 돼요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그런데 우리가 겪은 이야기들을 나누면서 우리는 외롭지 않았어요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우리가 서로 이해받는다고 느꼈으니까요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그렇게 각자 자신을 이해하는 과정이 있었어요.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 트라우마는 우리의 일부분이에요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우리가 받은 충격은 백 퍼센트 사라지지 않아요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그냥 조금씩 앞으로 걸어 나갈 수 있을 뿐이에요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 사실 지금도 힘들지만 더 이상 몇 년 전처럼 끔찍한 상태로 머물러 있지는 않아요.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우리가 곧잘 그 사실을 진지하게 여기지 않지만 세상은 이야기로 이루어져 있다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 언제나 가장 좋은 이야기로 힘을 내고, 가장 좋은 이야기와 함께 여러 가지 압력에 맞서 싸우면서 따뜻하면서도 깊게 대담하면서도 섬세하게 살 수 있게 된다면 기쁠 것이다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 현실을 살되 마음의 한쪽에 뭔가를 품고 현실의 일부분을 바꿀 수 있다면 기쁠 것이다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 저마다 이 문제 많은 현실의 ‘해결자의 목소리’가 된다면 기쁠 것이다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 우리가 가진 여러 모습 중 가장 좋은 모습이 우리의 미래가 된다면 정말 기쁠 것이다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 사람들은 아마 죽었다 깨어나도 서로를 이해하지 못할 거라고 생각한다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 알지 못하니 가질 수도 없다. ‘나’와 ‘너’, ‘우리’의 경계에서 빈손으로 헤맬 뿐이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 이것을 영원히 채워지지 않는 결핍으로 볼 수도 있겠지만, 나는 끝없는 가능성이라 말하고 싶다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 우리의 빈손은 잠시 악수를 나누는 동안 충만해진다고, 두 손바닥의 냉기가 맞닿아 온기가 되는 거라고 믿는다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 믿으려는 의지만으론 믿음이 생기지 않아 우리 모두 가끔은 미칠 때가 있다는 생각이 드는 요즘이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 스스로의 미침을 허용하는 인간만이 타인의 광기에도 조금쯤 유연할 수 있었다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 자기가 미쳤듯이 저 사람도 미쳤음을 이해하고, 그가 미칠 힘이 떨어져 제정 신이 되기를 기다려줄 수도 있기 때문이었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그렇게 따지면 나 한 몸 미쳐보는 일은 다시 가장 이타적인 행위가 되었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그래서 이제는 미쳤다는 소리를 착한 일 스티커처럼 모으고 있다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 내 마음속 빈칸이 숭숭 뚫린 판이 다 채워질 때마다 수고한 나 자신에게 약간 비싼 무언가를 사 준다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 모쪼록 이해받지 못할수록 즐거운 삶이라 생각하면서, 즐거움은 고단함의 다른 이름일지도 모르겠다고 얼버무리면서.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 사랑이든 미움이든, 끓는 감정에는 기다림이 필요한 법이었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 사랑이었다가 미움으로 둔갑한 마음이라면 더욱 그랬다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 두고 본 후에도 끓고 있다면 그때 온도를 확정해도 늦지 않았다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그제야 ‘시간의 힘’ 옆에 ‘빌린다’라 는 동사가 따르는 이유를 알 것 같았다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 시간은 내 것도 내 편도 아니지만, 언제나 나보다 힘이 셌다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그리고 너그러웠다.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 내가 빌리고자 한다면 이자를 붙이지 않고 여유를 내어줄 것이었다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : “넌 내가 한 마디를 하면 열 마디 백 마디를\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 해. 제발 좀 고분고분할 수 없어?”\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그때 나는 가여운 안구에서 그를 몰아내기 위해 눈을 감았다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 어제도 그제도 그끄제도?? 수없이 똥을 밟은 자리에서 이번에는 오바이트를 밟은 느낌이었다.\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 이렇게 자주 밟으면 이제는 길보다 밟는 이의 지성을 의심해야 할지 몰랐다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 이다지도 지루한 상황은 늘 약간의 욕설로만 새로울 수 있었다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : ‘습자지 같은 자식?? 또 팔랑거리는군.’\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 그러나 나는 그가 판단하는 것보단 세련되기 때문에 속마음과 다르게 입으로는 “자기야.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그건 네 빠그라진 자의식이 너 자신과 빚은 오해야.\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 근데 나와의 오해인 것처럼 말하면 너무 속상해”라고 울상을 할 줄 알았다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : “너무너무 죄송해요, 실수로 그만??”\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : “어우, 조심했어야지!\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 근데 너 이거 실수 맞지?”\n",
      "감정 : 분노\n",
      "\n",
      "문장 : “그럼요. 어떤 미친놈이 이 드러운 걸 일부러 쏟겠습니까?”\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 나는 결백해 보이려고 어금니까지 입을 찢고 웃었는데, 어쩐지 다음 날부터는 애 성격 또라이 같다는 소문이 돌았다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 근데 아무렇지도 않았다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 오히려 속이 시원했다.\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 삶의 속내를 짐작하는 과정에서, 복수에 대한 생각을 정리했다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 내가 꿈꾸는 복수는 죄다 범법이어서 이룰 수 없었고, 이루지 않는 게 나았다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그렇다면 타인을 죽이지 않으며 제거하는 방법을 배워야 했다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 내 생각에 방법은 잊는 것뿐이었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 망각을 용서의 개념으로 두면 해주기 싫기 때문에 두 가지를 분리했다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나는 절대로 용서하지 않으면서 다 잊었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 아무것도 용서되지 않기에 더 열심히 잊어버렸다. (…) 최후의 내가 천사가 된 것은 아니었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 난 그냥 인간이기 때문에 잊었다고 생각한 것들에 불시에 사로잡힐 때도 있었다. “생각해보니까 열 받네.” 혹은 “생각할수록 열 받아.” 연쇄적 데굴데굴 분노로, 여름에도 냉동고에 갇힌 듯한 감정을 느낄 수 있었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그럴 땐 내 삶보다 내게 상처 준 사람들의 삶을 믿었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그들이 그들이기 때문에 스스로 망쳐나갈 세월과 사건들을 기대했다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 최상의 산문 문장은 고통도 적확하게 묘파되면 달콤해진다는 것을 입증하는 문장이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 달콤한 고통이 무엇인지를 꿈과 잠의 주체인 우리는 안다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 꿈과 잠에 비유해본다면, 그녀의 문장은, 어떤 이유에서인지 한없이 눈물을 흘리다가 탈진한 상태로 깨어나서는 한참을 더 울게 되는 그런 꿈이고, 탈진한 상태로 깨어나서 한참을 더 울다가 사랑하는 사람의 품에 안겨 그 슬픔이 달콤한 안도감으로 서서히 바뀌는 것을 느끼는 순간 다시 찾아오는 그런 잠이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 자신의 진실을 충분히 설명하지 못한 채 규정되는 모든 존재들은 억울하다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 이 억울함이 벌써 폭력의 결과다. ‘폭력’의 외연은 가급적 넓히는 것이 좋다고 생각하면서 나는 이런 정의를 시도해본다. ‘폭력이란? 어떤 사람/사건의 진실에 최대한 섬세해지려는 노력을 포기하는 데서 만족을 얻는 모든 태도.’\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 한편 좋은 소설에서 인물들은 대개 비슷한 일을 겪는다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 문득 사건이 발생한다, 평범한 사람이 그 사건의 의미를 해석하느라 고뇌한다, 마침내 치명적인 진실을 손에 쥐고는 어찌할 바를 모르다가 자신이 더 이상 옛날로 돌아갈 수 없다는 사실을 깨닫는다, 이런 식이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그렇다면 유다는, 가장 사랑하는 대상을 배반해야만 그 사랑을 완성할 수 있는 상황에 처했던, 비극적인 인물이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 물론 신학적으로는 터무니없는 오독처럼 보일 것이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그러나 문학은 이 오독의 빛에 의지해 인간이라는 심해로 내려간다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나는 ‘소설적인 문장’이라는 것이 따로 있다고 믿는 편이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그저 아름답게 쓰면 된다는 뜻이 아니다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 요령부득의 문장을 써놓고 폼을 잡아야 한다는 뜻도 아니다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 동어반복처럼 들리겠지만, 소설적인 문장은 ‘소설적인 문장이란 무엇인가’라는 물음 속에서 고뇌한 흔적을 품고 있는 문장이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나는 아포리즘을 경멸하지 않는다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나로서는 중언부언과 지리멸렬이 차라리 더 견디기 힘들다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그러나 아포리즘이 시나 소설에서 반드시 긍정적인 역할을 한다고 단언할 수는 없다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 아포리즘 따위는 쓰지 않겠다는 고집이 오히려 독창적인 문학적 개성을 만들기도 한다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 인간의 깊은 곳까지 내려가서 그 어둠 속에 앉아 있어본 작가는 대낮의 햇살에서도 영혼을 느낄 것이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 내게 작품의 깊이란 곧 ‘인간 이해’의 깊이다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 아름다움이라는 말에 질색하고 시에서 그 가치를 수상쩍어하는 이들도 있지만, 나는 그들이 아름다움을 포기하고 얻은 것들에 조금도 질투를 느끼지 않는다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 시는 세계와 싸울 때조차도, 아름다움을 위해, 아름다움과 함께 싸워야 한다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 사랑받는 사람이 되는 가장 정확한 방법은 사랑받을 만한 사람이 되는 것이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 정확한 길이기는 하지만, 쉽고 빠른 길은 아니다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 사랑받을 만한 사람이 되기 위해서는 타인과의 섬세하고 복잡한 커뮤니케이션에 성공해야 한다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그 어렵고 느린 길을 걸을 능력도 의지도 없는 이들은 그 대신 권력을 가지려 한다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 권력을 얻어 명령의 주체가 되면 커뮤니케이션을 생략해도 된다고 믿기 때문이다.\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 왜 개새끼라고 하나. 개가 사람한테 너무 잘해줘서 그런 거 아닌가.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 아무 조건도 없이 잘해주니까, 때려도 피하지 않고 꼬리를 흔드니까, 복종하니까, 좋아하니까 그걸 도리어 우습게 보고 경멸하는 게 아닐까.\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 그런 게 사람 아닐까.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 나는 그 생각을 하며 개새끼라는 단어를 가만히 내려다봤다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나 자신이 개새끼 같았다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 마음이라는 것이 꺼내볼 수 있는 몸속 장기라면, 가끔 가슴에 손을 넣어 꺼내서 따뜻한 물로 씻어주고 싶었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 깨끗하게 씻어서 수건으로 물기를 닦고 해가 잘 들고 바람이 잘 통하는 곳에 널어놓고 싶었다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그러는 동안 나는 마음이 없는 사람으로 살고, 마음이 햇볕에 잘 마르면 부드럽고 좋은 향기가 나는 마음을 다시 가슴에 넣고 새롭게 시작할 수 있겠지.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 증조모는 뒤도 돌아보지 않고 집을 나왔다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 잠시라도 뒤돌아보면 떠날 수 없을 것 같아서였다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 십칠 년 동안 살던 집, 누린내가 가시지 않던 집, 똥지게꾼도 상대해주지 않아 스스로 오물을 퍼내야 했던 집, 해질녘 구석에 핀 꽃이 예뻐 바라보다 아무 이유도 없이 날아온 돌에 머리를 맞아야 했던, 무엇 하나 좋은 기억이 없던 집. 그 집을 떠나 기차역으로 가는데 그 짧은 길이 천릿길 같았고, 걸음걸음이 무거워 납으로 만든 신발을 신은 것 같았다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그래도 떠나야 했다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그게 사는 길이었으니까.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그녀에게는 그런 재능이 있었다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 어떤 경우에도 자신을 속이지 않는 재능. 부당한 일은 부당한 일로, 슬픈 일은 슬픈 일로, 외로운 마음은 외로운 마음으로 느끼는 재능.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그녀는 아이가 작은 몸과 마음으로 눈치를 살피느라 마음껏 울어보지도 못하는 게 아닐지 근심했다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그녀의 사랑은 그 근심에서 자랐다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 아이와 눈을 마주치며 웃던 어느 날, 그녀는 자신이 아이를 마음으로 귀하게 여기고 있음을 알았다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그것이 세상 사람들이 말하는 어미의 본능적 사랑 같은 것은 아닐지 몰라도.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그렇지만 그게 다 무슨 소용일까.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 사람이 사람을 기억하는 일, 이 세상에 머물다 사라진 누군가를 기억한다는 것이 무슨 의미가 있을지 알 수 없었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 나는 기억되고 싶을까.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 나 자신에게 물어보면 언제나 답은 기억되고 싶지 않다는 것이었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 내가 기원하든 그러지 않든 그것이 인간의 최종 결말이기도 했다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 지구가 수명을 다하고, 그보다 더 긴 시간이 지나 엔트로피가 최대가 되는 순간이 오면 시간마저도 사라지게 된다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그때 인간은 그들이 잠시 우주에 머물렀다는 사실조차도 기억되지 못하는 종족이 된다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우주는 그들을 기억할 수 있는 마음이 없는 곳이 된다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그것이 우리의 최종 결말이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 삼천아, 새비에는 지금 진달래가 한창이야.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 개성도 그렇니.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 너랑 같이 꽃을 뽑아다가 꿀을 먹던 게 생각나.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그걸 따다가 전을 부쳐 먹던 것두, 같이 쑥을 캐다가 떡을 만들어 먹던 것도. 인제 나는 꽃을 봐도 풀을 봐도 네 생각을 하는 사람이 됐어.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 별을 봐도 달을 봐도 그걸 올려다보던 삼천이 네 얼굴만 떠올라.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 새비야, 참 희한하지 않아?\n",
      "감정 : 분노\n",
      "\n",
      "문장 : 밤하늘을 보면서 그리 말하던 네가 떠올라.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 이것도 희한하구 저것도 희한한 우리 삼천이가 생각나누나.\n",
      "감정 : 행복\n",
      "\n",
      "문장 : 우리는 둥글고 푸른 배를 타고 컴컴한 바다를 떠돌다 대부분 백 년도 되지 않아 떠나야 한다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그래서 어디로 가나.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 나는 종종 그런 생각을 했다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 우주의 나이에 비한다면, 아니, 그보다 훨씬 짧은 지구의 나이에 비한다고 하더라도 우리의 삶은 너무도 찰나가 아닐까.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 찰나에 불과한 삶이 왜 때로는 이렇게 길고 고통스럽게 느껴지는 것인지 이해할 수 없었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 참나무로, 기러기로 태어날 수도 있었을 텐데, 어째서 인간이었던 걸까.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 원자폭탄으로 그 많은 사람을 찢어 죽이고자 한 마음과 그 마음을 실행으로 옮긴 힘은 모두 인간에게서 나왔다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 나는 그들과 같은 인간이다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 별의 먼지로 만들어진 인간이 빚어내는 고통에 대해, 별의 먼지가 어떻게 배열되었기에 인간 존재가 되었는지에 대해 가만히 생각했다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 언젠가 별이었을, 그리고 언젠가는 초신성의 파편이었을 나의 몸을 만져보면서. 모든 것이 새삼스러웠다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 예전처럼 며칠씩 서로 말도 붙이지 않을 정도로 신경전을 벌일 만한 일이 우리에게는 더이상 없었다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 큰불이 나기 전에 꺼버렸고, 상대에게 작은 불씨를 던졌다는 것에 문득 무안해지기도 하는 사이가 된 것이었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 그건 우리가 그만큼 친밀한 사이가 아니라는 뜻이기도 했다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 서로에게 큰 상처를 입혔다가 돌이킬 수 없게 될지도 모른다는 두려움을 우리는 눈빛으로 공유하고 있었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 우리는 더이상 끝까지 싸울 수 없는 사이가 되었다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 정말 끝이 날까봐 끝까지 싸울 수 없는 사이가.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 당장이라도 무슨 일이 터질 것 같다는 생각을 하며 전전긍긍할 때는 별다른 일이 없다가도 조금이라도 안심하면 뒤통수를 치는 것이 삶이라고 할머니는 생각했다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 불행은 그런 환경을 좋아하는 것 같았다.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 겨우 한숨 돌렸을 때, 이제는 좀 살아볼 만한가보다 생각할 때.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 편지에서 묻어 나오는 명숙 할머니의 애정이 할머니는 버거웠다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 명숙 할머니의 편지를 읽다보면 결국 자신이 누군가에게 사랑받고 싶어하는 사람이라는 것을 알게 됐으니까.\n",
      "감정 : 중립\n",
      "\n",
      "문장 : 그것도 아주 간절하고 절실하게, 사랑받고 싶어하는 사람이라는 것을 인정하게 됐으니까.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 남선의 모진 말들은 얼마든지 견딜 수가 있었다. 하지만 명숙 할머니의 편지를 읽으면 늘 마음이 아팠다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 사랑은 할머니를 울게 했다.\n",
      "감정 : 슬픔\n",
      "\n",
      "문장 : 모욕이나 상처조차도 건드리지 못한 마음을 건드렸다.\n",
      "감정 : 슬픔\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent, emotion in sents_df.values:\n",
    "    print(f'문장 : {sent}\\n감정 : {emotion}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c4ed0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df = sents_df[~sents_df['감정'].str.contains(\"중립\", na=False, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2862753e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "슬픔    3386\n",
       "분노    1198\n",
       "행복     740\n",
       "Name: 감정, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_df.감정.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e1c01b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df.to_csv(\"../train data/train_gpt2_sent.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
